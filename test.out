============================= test session starts ==============================
platform linux -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0 -- /home/jscharber/eng/deeplake-api/.venv/bin/python3
cachedir: .pytest_cache
rootdir: /home/jscharber/eng/deeplake-api
configfile: pyproject.toml
plugins: cov-6.2.1, asyncio-1.0.0, anyio-4.9.0
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 60 items

tests/integration/test_end_to_end.py::TestEndToEnd::test_complete_workflow FAILED [  1%]
tests/integration/test_end_to_end.py::TestEndToEnd::test_error_scenarios FAILED [  3%]
tests/integration/test_end_to_end.py::TestEndToEnd::test_authentication_flows FAILED [  5%]
tests/integration/test_end_to_end.py::TestEndToEnd::test_tenant_isolation FAILED [  6%]
tests/integration/test_end_to_end.py::TestPerformance::test_large_batch_insert FAILED [  8%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_successful_single_vector_insert FAILED [ 10%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_vector_insert_with_metadata FAILED [ 11%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_vector_insert_with_custom_id FAILED [ 13%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_invalid_vector_dimensions PASSED [ 15%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_missing_required_fields PASSED [ 16%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_invalid_vector_values PASSED [ 18%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_debugger_trigger_invalid_data PASSED [ 20%]
tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_nonexistent_dataset PASSED [ 21%]
tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_multiple_vectors FAILED [ 23%]
tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_with_skip_existing FAILED [ 25%]
tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_mixed_valid_invalid FAILED [ 26%]
tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_unauthorized PASSED [ 28%]
tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_invalid_auth PASSED [ 30%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_malformed_json PASSED [ 31%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_empty_request_body PASSED [ 33%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_invalid_content_type PASSED [ 35%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_vector_values_wrong_type PASSED [ 36%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_vector_values_empty_array PASSED [ 38%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_large_metadata_object PASSED [ 40%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_vector_with_null_values PASSED [ 41%]
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_concurrent_inserts PASSED [ 43%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_create_dataset PASSED [ 45%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_create_duplicate_dataset PASSED [ 46%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_get_dataset PASSED [ 48%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_get_nonexistent_dataset PASSED [ 50%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_list_datasets FAILED [ 51%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_delete_dataset PASSED [ 53%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_insert_vectors FAILED [ 55%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_insert_vector_wrong_dimensions FAILED [ 56%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_search_vectors FAILED [ 58%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_search_nonexistent_dataset PASSED [ 60%]
tests/unit/test_deeplake_service.py::TestDeepLakeService::test_tenant_isolation PASSED [ 61%]
tests/unit/test_http_api.py::TestHealthEndpoints::test_health_check PASSED [ 63%]
tests/unit/test_http_api.py::TestHealthEndpoints::test_liveness_check PASSED [ 65%]
tests/unit/test_http_api.py::TestHealthEndpoints::test_readiness_check PASSED [ 66%]
tests/unit/test_http_api.py::TestDatasetEndpoints::test_create_dataset_unauthorized FAILED [ 68%]
tests/unit/test_http_api.py::TestDatasetEndpoints::test_create_dataset_authorized PASSED [ 70%]
tests/unit/test_http_api.py::TestDatasetEndpoints::test_list_datasets_authorized PASSED [ 71%]
tests/unit/test_http_api.py::TestDatasetEndpoints::test_get_dataset_not_found PASSED [ 73%]
tests/unit/test_http_api.py::TestDatasetEndpoints::test_dataset_lifecycle PASSED [ 75%]
tests/unit/test_http_api.py::TestVectorEndpoints::test_insert_vector_unauthorized FAILED [ 76%]
tests/unit/test_http_api.py::TestVectorEndpoints::test_insert_vector_nonexistent_dataset FAILED [ 78%]
tests/unit/test_http_api.py::TestVectorEndpoints::test_vector_operations_with_dataset FAILED [ 80%]
tests/unit/test_http_api.py::TestSearchEndpoints::test_search_unauthorized FAILED [ 81%]
tests/unit/test_http_api.py::TestSearchEndpoints::test_search_nonexistent_dataset FAILED [ 83%]
tests/unit/test_http_api.py::TestSearchEndpoints::test_search_with_dataset FAILED [ 85%]
tests/unit/test_http_api.py::TestSearchEndpoints::test_text_search_not_implemented FAILED [ 86%]
tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_no_auth_header FAILED [ 88%]
tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_invalid_auth_header PASSED [ 90%]
tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_invalid_api_key PASSED [ 91%]
tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_valid_jwt_token PASSED [ 93%]
tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_admin_only_endpoint FAILED [ 95%]
tests/unit/test_http_api.py::TestErrorHandling::test_invalid_json PASSED [ 96%]
tests/unit/test_http_api.py::TestErrorHandling::test_validation_errors PASSED [ 98%]
tests/unit/test_http_api.py::TestErrorHandling::test_request_id_header PASSED [100%]

=================================== FAILURES ===================================
_____________________ TestEndToEnd.test_complete_workflow ______________________

self = <tests.integration.test_end_to_end.TestEndToEnd object at 0x7666fd52c050>
client = <starlette.testclient.TestClient object at 0x7666fd5a1400>
auth_headers = {'Authorization': 'ApiKey tznZzxhhGy7nnBtISymrohs-mPVC5BCFmXIX9LZcBkY', 'Content-Type': 'application/json'}

    def test_complete_workflow(self, client: TestClient, auth_headers):
        """Test complete workflow from dataset creation to search."""
    
        # 1. Create a dataset
        dataset_data = {
            "name": "e2e-test-dataset",
            "description": "End-to-end test dataset",
            "dimensions": 128,
            "metric_type": "cosine",
            "index_type": "default",
            "metadata": {"test": "e2e"},
            "overwrite": True
        }
    
        create_response = client.post(
            "/api/v1/datasets/",
            json=dataset_data,
            headers=auth_headers
        )
        assert create_response.status_code == 201
        dataset = create_response.json()
        dataset_id = dataset["id"]
    
        # 2. Verify dataset was created
        get_response = client.get(f"/api/v1/datasets/{dataset_id}", headers=auth_headers)
        assert get_response.status_code == 200
        assert get_response.json()["name"] == dataset_data["name"]
    
        # 3. Insert multiple vectors
        vectors = []
        for i in range(5):
            vector = {
                "id": f"test-vector-{i}",
                "document_id": f"test-doc-{i}",
                "chunk_id": f"test-chunk-{i}",
                "values": [0.1 + i * 0.01] * 128,  # Slightly different vectors
                "content": f"This is test content {i}",
                "metadata": {"index": str(i), "test": "e2e"},
                "content_type": "text/plain",
                "language": "en",
                "chunk_index": 0,
                "chunk_count": 1,
                "model": "test-model"
            }
            vectors.append(vector)
    
        batch_data = {"vectors": vectors}
        insert_response = client.post(
            f"/api/v1/datasets/{dataset_id}/vectors/batch",
            json=batch_data,
            headers=auth_headers
        )
        assert insert_response.status_code == 201
        insert_result = insert_response.json()
>       assert insert_result["inserted_count"] == 5
E       assert 0 == 5

tests/integration/test_end_to_end.py:66: AssertionError
______________________ TestEndToEnd.test_error_scenarios _______________________

self = <tests.integration.test_end_to_end.TestEndToEnd object at 0x7666fd52c550>
client = <starlette.testclient.TestClient object at 0x7666fd5bc550>
auth_headers = {'Authorization': 'ApiKey T69fP3dXPXp9rLDnU6vGdWMFObu6o0cnzQ4VsQSSIys', 'Content-Type': 'application/json'}

    def test_error_scenarios(self, client: TestClient, auth_headers):
        """Test various error scenarios."""
    
        # Test creating dataset with invalid data
        invalid_dataset = {
            "name": "",  # Empty name
            "dimensions": 0,  # Invalid dimensions
            "metric_type": "invalid_metric"
        }
    
        response = client.post(
            "/api/v1/datasets/",
            json=invalid_dataset,
            headers=auth_headers
        )
        assert response.status_code == 422
    
        # Test operations on non-existent dataset
        response = client.get("/api/v1/datasets/nonexistent", headers=auth_headers)
        assert response.status_code == 404
    
        response = client.post(
            "/api/v1/datasets/nonexistent/vectors/",
            json={"id": "test", "document_id": "test", "values": [0.1]},
            headers=auth_headers
        )
>       assert response.status_code == 404
E       assert 201 == 404
E        +  where 201 = <Response [201 Created]>.status_code

tests/integration/test_end_to_end.py:146: AssertionError
____________________ TestEndToEnd.test_authentication_flows ____________________

self = <tests.integration.test_end_to_end.TestEndToEnd object at 0x7666fd518b00>
client = <starlette.testclient.TestClient object at 0x7666fd462210>
auth_service = <app.services.auth_service.AuthService object at 0x7666fd462990>

    def test_authentication_flows(self, client: TestClient, auth_service):
        """Test different authentication methods."""
    
        # Test API key authentication
        api_key = auth_service.generate_api_key(
            tenant_id="test-tenant",
            name="Test API Key",
            permissions=["read", "write"]
        )
    
        api_key_headers = {
            "Authorization": f"ApiKey {api_key}",
            "Content-Type": "application/json"
        }
    
        response = client.get("/api/v1/datasets/", headers=api_key_headers)
>       assert response.status_code == 200
E       assert 403 == 200
E        +  where 403 = <Response [403 Forbidden]>.status_code

tests/integration/test_end_to_end.py:171: AssertionError
______________________ TestEndToEnd.test_tenant_isolation ______________________

self = <tests.integration.test_end_to_end.TestEndToEnd object at 0x7666fd518d60>
client = <starlette.testclient.TestClient object at 0x7666fd456780>
auth_service = <app.services.auth_service.AuthService object at 0x7666fd457490>

    def test_tenant_isolation(self, client: TestClient, auth_service):
        """Test that tenants are properly isolated."""
    
        # Create API keys for different tenants
        tenant1_key = auth_service.generate_api_key(
            tenant_id="tenant1",
            name="Tenant 1 Key",
            permissions=["read", "write"]
        )
    
        tenant2_key = auth_service.generate_api_key(
            tenant_id="tenant2",
            name="Tenant 2 Key",
            permissions=["read", "write"]
        )
    
        tenant1_headers = {"Authorization": f"ApiKey {tenant1_key}"}
        tenant2_headers = {"Authorization": f"ApiKey {tenant2_key}"}
    
        # Create dataset for tenant1
        dataset_data = {
            "name": "tenant1-dataset",
            "dimensions": 64,
            "metric_type": "cosine",
            "overwrite": True
        }
    
        response = client.post(
            "/api/v1/datasets/",
            json=dataset_data,
            headers=tenant1_headers
        )
>       assert response.status_code == 201
E       assert 403 == 201
E        +  where 403 = <Response [403 Forbidden]>.status_code

tests/integration/test_end_to_end.py:226: AssertionError
___________________ TestPerformance.test_large_batch_insert ____________________

self = <tests.integration.test_end_to_end.TestPerformance object at 0x7666fd52c410>
client = <starlette.testclient.TestClient object at 0x7666fd456c40>
auth_headers = {'Authorization': 'ApiKey BHeBLv6lOu3_BqRUjbChOdFl-5tRVZxsrtSHDUVdbRs', 'Content-Type': 'application/json'}

    def test_large_batch_insert(self, client: TestClient, auth_headers):
        """Test inserting a large batch of vectors."""
    
        # Create dataset
        dataset_data = {
            "name": "performance-test",
            "dimensions": 256,
            "metric_type": "cosine",
            "overwrite": True
        }
    
        response = client.post(
            "/api/v1/datasets/",
            json=dataset_data,
            headers=auth_headers
        )
>       assert response.status_code == 201
E       assert 400 == 201
E        +  where 400 = <Response [400 Bad Request]>.status_code

tests/integration/test_end_to_end.py:267: AssertionError
----------------------------- Captured stdout call -----------------------------
{"name": "performance-test", "error": "Array(): incompatible function arguments. The following argument types are supported:\n    1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType\n    2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType\n    3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType\n\nInvoked with: <deeplake._deeplake.types.DataType object at 0x7666fd4db7b0>; kwargs: dimensions=256", "event": "Failed to create dataset", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:03.697533Z"}
------------------------------ Captured log call -------------------------------
ERROR    DeepLakeService:deeplake_service.py:188 {"name": "performance-test", "error": "Array(): incompatible function arguments. The following argument types are supported:\n    1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType\n    2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType\n    3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType\n\nInvoked with: <deeplake._deeplake.types.DataType object at 0x7666fd4db7b0>; kwargs: dimensions=256", "event": "Failed to create dataset", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:03.697533Z"}
________ TestVectorInsertEndpoints.test_successful_single_vector_insert ________

self = <tests.integration.test_vector_insert_api.TestVectorInsertEndpoints object at 0x7666fd52c690>
client = <starlette.testclient.TestClient object at 0x7666fd4b4a70>
test_dataset_3d = 'test-dataset-3d'
auth_headers = {'Authorization': 'ApiKey fD3ihnkkYzq0aoWlCW-lZ_rAfzz8Vt8DY1FsiHa-nE8', 'Content-Type': 'application/json'}

    def test_successful_single_vector_insert(self, client: TestClient, test_dataset_3d: str, auth_headers: Dict[str, str]):
        """Test successful single vector insert."""
        vector_data = {
            "document_id": "doc-1",
            "values": [1.0, 0.5, 0.0],
            "chunk_count": 1
        }
    
        response = client.post(
            f"/api/v1/datasets/{test_dataset_3d}/vectors/",
            json=vector_data,
            headers=auth_headers
        )
    
        assert response.status_code == 201
        data = response.json()
        assert data["success"] is True
>       assert data["inserted_count"] == 1
E       assert 0 == 1

tests/integration/test_vector_insert_api.py:50: AssertionError
__________ TestVectorInsertEndpoints.test_vector_insert_with_metadata __________

self = <tests.integration.test_vector_insert_api.TestVectorInsertEndpoints object at 0x7666fd52c7d0>
client = <starlette.testclient.TestClient object at 0x7666fd4b96a0>
test_dataset_3d = 'test-dataset-3d'
auth_headers = {'Authorization': 'ApiKey _8U0X2AoTTs1QC_kUUdFsAddLcSbzhb3RKIUERlikls', 'Content-Type': 'application/json'}

    def test_vector_insert_with_metadata(self, client: TestClient, test_dataset_3d: str, auth_headers: Dict[str, str]):
        """Test vector insert with metadata."""
        vector_data = {
            "document_id": "doc-2",
            "values": [0.8, 0.2, 0.1],
            "chunk_count": 1,
            "content": "This is sample content",
            "metadata": {
                "category": "test",
                "priority": "high"
            }
        }
    
        response = client.post(
            f"/api/v1/datasets/{test_dataset_3d}/vectors/",
            json=vector_data,
            headers=auth_headers
        )
    
        assert response.status_code == 201
        data = response.json()
        assert data["success"] is True
>       assert data["inserted_count"] == 1
E       assert 0 == 1

tests/integration/test_vector_insert_api.py:76: AssertionError
_________ TestVectorInsertEndpoints.test_vector_insert_with_custom_id __________

self = <tests.integration.test_vector_insert_api.TestVectorInsertEndpoints object at 0x7666fd518c30>
client = <starlette.testclient.TestClient object at 0x7666fd4b9ae0>
test_dataset_3d = 'test-dataset-3d'
auth_headers = {'Authorization': 'ApiKey cFyxhmEd38ZsHtDgKBugIeNQ1TWHOTTZIdB78LAhwcg', 'Content-Type': 'application/json'}

    def test_vector_insert_with_custom_id(self, client: TestClient, test_dataset_3d: str, auth_headers: Dict[str, str]):
        """Test vector insert with custom ID."""
        vector_data = {
            "id": "custom-vector-123",
            "document_id": "doc-3",
            "values": [0.3, 0.7, 0.9],
            "chunk_count": 1
        }
    
        response = client.post(
            f"/api/v1/datasets/{test_dataset_3d}/vectors/",
            json=vector_data,
            headers=auth_headers
        )
    
        assert response.status_code == 201
        data = response.json()
        assert data["success"] is True
>       assert data["inserted_count"] == 1
E       assert 0 == 1

tests/integration/test_vector_insert_api.py:96: AssertionError
______ TestVectorBatchInsertEndpoints.test_batch_insert_multiple_vectors _______

self = <tests.integration.test_vector_insert_api.TestVectorBatchInsertEndpoints object at 0x7666fd52c2d0>
client = <starlette.testclient.TestClient object at 0x7666fd46ba10>
test_dataset_3d_batch = 'test-dataset-batch'
auth_headers = {'Authorization': 'ApiKey wS-oKl9TJe91CytxOLvYR-7TdiJIMBf3UeIOJfHnQMA', 'Content-Type': 'application/json'}

    def test_batch_insert_multiple_vectors(self, client: TestClient, test_dataset_3d_batch: str, auth_headers: Dict[str, str]):
        """Test batch insert with multiple vectors."""
        batch_data = {
            "vectors": [
                {"document_id": "batch-1", "values": [1.0, 0.0, 0.0], "chunk_count": 1},
                {"document_id": "batch-2", "values": [0.0, 1.0, 0.0], "chunk_count": 1},
                {"document_id": "batch-3", "values": [0.0, 0.0, 1.0], "chunk_count": 1}
            ],
            "skip_existing": False,
            "overwrite": False
        }
    
        response = client.post(
            f"/api/v1/datasets/{test_dataset_3d_batch}/vectors/batch",
            json=batch_data,
            headers=auth_headers
        )
    
        assert response.status_code == 201
        data = response.json()
>       assert data["inserted_count"] == 3
E       assert 0 == 3

tests/integration/test_vector_insert_api.py:232: AssertionError
_____ TestVectorBatchInsertEndpoints.test_batch_insert_with_skip_existing ______

self = <tests.integration.test_vector_insert_api.TestVectorBatchInsertEndpoints object at 0x7666fd52c910>
client = <starlette.testclient.TestClient object at 0x7666fc344d70>
test_dataset_3d_batch = 'test-dataset-batch'
auth_headers = {'Authorization': 'ApiKey nKZg3iOH_ZsYZVC9oZ4jVdz3_q2iv0yMux-4A0ZxaSM', 'Content-Type': 'application/json'}

    def test_batch_insert_with_skip_existing(self, client: TestClient, test_dataset_3d_batch: str, auth_headers: Dict[str, str]):
        """Test batch insert with skip_existing option."""
        # First insert a vector
        initial_batch = {
            "vectors": [
                {"id": "duplicate-vector", "document_id": "batch-4", "values": [0.5, 0.5, 0.5], "chunk_count": 1}
            ],
            "skip_existing": False,
            "overwrite": False
        }
    
        response1 = client.post(
            f"/api/v1/datasets/{test_dataset_3d_batch}/vectors/batch",
            json=initial_batch,
            headers=auth_headers
        )
    
        assert response1.status_code == 201
    
        # Try to insert same vector with skip_existing=True
        duplicate_batch = {
            "vectors": [
                {"id": "duplicate-vector", "document_id": "batch-4", "values": [0.5, 0.5, 0.5], "chunk_count": 1}
            ],
            "skip_existing": True,
            "overwrite": False
        }
    
        response2 = client.post(
            f"/api/v1/datasets/{test_dataset_3d_batch}/vectors/batch",
            json=duplicate_batch,
            headers=auth_headers
        )
    
        assert response2.status_code == 201
        data = response2.json()
        # Note: skip_existing functionality may not be fully implemented
        # So we'll accept either behavior for now
>       assert data["inserted_count"] + data["skipped_count"] == 1
E       assert (0 + 0) == 1

tests/integration/test_vector_insert_api.py:274: AssertionError
_____ TestVectorBatchInsertEndpoints.test_batch_insert_mixed_valid_invalid _____

self = <tests.integration.test_vector_insert_api.TestVectorBatchInsertEndpoints object at 0x7666fd518e90>
client = <starlette.testclient.TestClient object at 0x7666fd4aa0b0>
test_dataset_3d_batch = 'test-dataset-batch'
auth_headers = {'Authorization': 'ApiKey 7VZBxdC1xLV7ZUvdwyC8i1OZbliP7nL-UZLPqTf7h3s', 'Content-Type': 'application/json'}

    def test_batch_insert_mixed_valid_invalid(self, client: TestClient, test_dataset_3d_batch: str, auth_headers: Dict[str, str]):
        """Test batch insert with mix of valid and invalid vectors."""
        batch_data = {
            "vectors": [
                {"document_id": "valid-1", "values": [1.0, 0.5, 0.0], "chunk_count": 1},
                {"document_id": "invalid-dim", "values": [1.0, 0.5], "chunk_count": 1},  # Invalid dimensions
                {"document_id": "valid-2", "values": [0.0, 0.5, 1.0], "chunk_count": 1}
            ],
            "skip_existing": False,
            "overwrite": False
        }
    
        response = client.post(
            f"/api/v1/datasets/{test_dataset_3d_batch}/vectors/batch",
            json=batch_data,
            headers=auth_headers
        )
    
        assert response.status_code == 201
        data = response.json()
>       assert data["inserted_count"] == 2
E       assert 0 == 2

tests/integration/test_vector_insert_api.py:296: AssertionError
____________________ TestDeepLakeService.test_list_datasets ____________________

self = <tests.unit.test_deeplake_service.TestDeepLakeService object at 0x7666fd5683b0>
deeplake_service = <app.services.deeplake_service.DeepLakeService object at 0x7666fc3971d0>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}

    async def test_list_datasets(self, deeplake_service: DeepLakeService, test_dataset_data):
        """Test listing datasets."""
        # Initially empty
        datasets = await deeplake_service.list_datasets("default")
        initial_count = len(datasets)
    
        # Create a dataset
        dataset_create = DatasetCreate(**test_dataset_data)
        await deeplake_service.create_dataset(dataset_create, "default")
    
        # Check list again
        datasets = await deeplake_service.list_datasets("default")
>       assert len(datasets) == initial_count + 1
E       assert 0 == (0 + 1)
E        +  where 0 = len([])

tests/unit/test_deeplake_service.py:69: AssertionError
___________________ TestDeepLakeService.test_insert_vectors ____________________

self = <tests.unit.test_deeplake_service.TestDeepLakeService object at 0x7666fd663df0>
deeplake_service = <app.services.deeplake_service.DeepLakeService object at 0x7666fd4c6ad0>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}

    async def test_insert_vectors(self, deeplake_service: DeepLakeService, test_dataset_data, test_vector_data):
        """Test vector insertion."""
        # Create dataset first
        dataset_create = DatasetCreate(**test_dataset_data)
        dataset = await deeplake_service.create_dataset(dataset_create, "default")
    
        # Insert vector
        vector_create = VectorCreate(**test_vector_data)
        result = await deeplake_service.insert_vectors(
            dataset_id=dataset.id,
            vectors=[vector_create],
            tenant_id="default"
        )
    
>       assert result.inserted_count == 1
E       assert 0 == 1
E        +  where 0 = VectorBatchResponse(inserted_count=0, skipped_count=0, failed_count=1, error_messages=["Vector test-vector-1: Invalid value for column 'embedding'. Reason - 'Data must have 128 dimensions provided 1'"], processing_time_ms=2.299785614013672).inserted_count

tests/unit/test_deeplake_service.py:98: AssertionError
___________ TestDeepLakeService.test_insert_vector_wrong_dimensions ____________

self = <app.services.deeplake_service.DeepLakeService object at 0x7666fc2de850>
dataset_create = DatasetCreate(name='test-dataset', description='A test dataset for unit tests', dimensions=256, metric_type='cosine', index_type='default', metadata={'test': 'true'}, storage_location=None, overwrite=True)
tenant_id = 'default'

    async def create_dataset(
        self,
        dataset_create: DatasetCreate,
        tenant_id: Optional[str] = None
    ) -> DatasetResponse:
        """Create a new Deep Lake dataset."""
        dataset_key = self._get_dataset_key(dataset_create.name, tenant_id)
        dataset_path = self._get_dataset_path(dataset_create.name, tenant_id)
    
        self.logger.info(
            "Creating dataset",
            name=dataset_create.name,
            tenant_id=tenant_id,
            path=dataset_path
        )
    
        # Check if dataset already exists
        if os.path.exists(dataset_path) and not dataset_create.overwrite:
            raise DatasetAlreadyExistsException(dataset_create.name, tenant_id)
    
        # Always clean up existing dataset to avoid corruption
        if os.path.exists(dataset_path):
            import shutil
            shutil.rmtree(dataset_path, ignore_errors=True)
            self.logger.info(f"Cleaned up existing dataset: {dataset_path}")
    
        try:
            # Create dataset directory if needed
            os.makedirs(os.path.dirname(dataset_path), exist_ok=True)
    
            # Dataset cleanup already handled above
    
            # Create Deep Lake dataset with 4.0 API using schema
            loop = asyncio.get_event_loop()
            # Only pass token if it's not None or empty
            create_kwargs: Dict[str, Any] = {}
            if self.token:
                create_kwargs["token"] = self.token
    
            # Define simplified schema for Deep Lake 4.0 compatibility
            import deeplake
            # Schema matching the corrected payload format
            schema = {
                'id': deeplake.types.Text(),
                'document_id': deeplake.types.Text(),
>               'embedding': deeplake.types.Array(deeplake.types.Float32(), dimensions=dataset_create.dimensions),
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                'content': deeplake.types.Text(),
                'chunk_count': deeplake.types.Int32()
            }
E           TypeError: Array(): incompatible function arguments. The following argument types are supported:
E               1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType
E               2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType
E               3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType
E           
E           Invoked with: <deeplake._deeplake.types.DataType object at 0x7666fc2f65f0>; kwargs: dimensions=256

app/services/deeplake_service.py:132: TypeError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_deeplake_service.TestDeepLakeService object at 0x7666fd6a6e50>
deeplake_service = <app.services.deeplake_service.DeepLakeService object at 0x7666fc2de850>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}

    async def test_insert_vector_wrong_dimensions(self, deeplake_service: DeepLakeService, test_dataset_data, test_vector_data):
        """Test inserting vector with wrong dimensions."""
        # Create dataset with specific dimensions
        dataset_create = DatasetCreate(**test_dataset_data)
        dataset_create.dimensions = 256  # Different from test vector
>       dataset = await deeplake_service.create_dataset(dataset_create, "default")
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/unit/test_deeplake_service.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <app.services.deeplake_service.DeepLakeService object at 0x7666fc2de850>
dataset_create = DatasetCreate(name='test-dataset', description='A test dataset for unit tests', dimensions=256, metric_type='cosine', index_type='default', metadata={'test': 'true'}, storage_location=None, overwrite=True)
tenant_id = 'default'

    async def create_dataset(
        self,
        dataset_create: DatasetCreate,
        tenant_id: Optional[str] = None
    ) -> DatasetResponse:
        """Create a new Deep Lake dataset."""
        dataset_key = self._get_dataset_key(dataset_create.name, tenant_id)
        dataset_path = self._get_dataset_path(dataset_create.name, tenant_id)
    
        self.logger.info(
            "Creating dataset",
            name=dataset_create.name,
            tenant_id=tenant_id,
            path=dataset_path
        )
    
        # Check if dataset already exists
        if os.path.exists(dataset_path) and not dataset_create.overwrite:
            raise DatasetAlreadyExistsException(dataset_create.name, tenant_id)
    
        # Always clean up existing dataset to avoid corruption
        if os.path.exists(dataset_path):
            import shutil
            shutil.rmtree(dataset_path, ignore_errors=True)
            self.logger.info(f"Cleaned up existing dataset: {dataset_path}")
    
        try:
            # Create dataset directory if needed
            os.makedirs(os.path.dirname(dataset_path), exist_ok=True)
    
            # Dataset cleanup already handled above
    
            # Create Deep Lake dataset with 4.0 API using schema
            loop = asyncio.get_event_loop()
            # Only pass token if it's not None or empty
            create_kwargs: Dict[str, Any] = {}
            if self.token:
                create_kwargs["token"] = self.token
    
            # Define simplified schema for Deep Lake 4.0 compatibility
            import deeplake
            # Schema matching the corrected payload format
            schema = {
                'id': deeplake.types.Text(),
                'document_id': deeplake.types.Text(),
                'embedding': deeplake.types.Array(deeplake.types.Float32(), dimensions=dataset_create.dimensions),
                'content': deeplake.types.Text(),
                'chunk_count': deeplake.types.Int32()
            }
    
            create_kwargs["schema"] = schema
    
            dataset = await loop.run_in_executor(
                self.executor,
                lambda: deeplake.create(dataset_path, **create_kwargs)
            )
    
            # Store dataset metadata in our own tracking (Deep Lake 4.0 doesn't use .info)
            dataset_metadata = {
                'name': dataset_create.name,
                'description': dataset_create.description or '',
                'dimensions': dataset_create.dimensions,
                'metric_type': dataset_create.metric_type,
                'index_type': dataset_create.index_type,
                'tenant_id': tenant_id or '',
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat(),
            }
    
            # Add custom metadata
            if dataset_create.metadata:
                dataset_metadata.update(dataset_create.metadata)
    
            # Store metadata in a JSON file alongside the dataset
            metadata_path = os.path.join(dataset_path, 'dataset_metadata.json')
            with open(metadata_path, 'w') as f:
                import json
                json.dump(dataset_metadata, f, indent=2)
    
            # Cache the dataset
            self.datasets[dataset_key] = dataset
    
            self.logger.info("Dataset created successfully", name=dataset_create.name)
    
            return DatasetResponse(
                id=dataset_create.name,
                name=dataset_create.name,
                description=dataset_create.description,
                dimensions=dataset_create.dimensions,
                metric_type=dataset_create.metric_type,
                index_type=dataset_create.index_type,
                metadata=dataset_create.metadata or {},
                storage_location=dataset_path,
                vector_count=0,
                storage_size=0,
                created_at=datetime.now(timezone.utc),
                updated_at=datetime.now(timezone.utc),
                tenant_id=tenant_id
            )
    
        except Exception as e:
            self.logger.error("Failed to create dataset", name=dataset_create.name, error=str(e))
>           raise StorageException(f"Failed to create dataset: {str(e)}", "create_dataset")
E           app.models.exceptions.StorageException: Failed to create dataset: Array(): incompatible function arguments. The following argument types are supported:
E               1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType
E               2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType
E               3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType
E           
E           Invoked with: <deeplake._deeplake.types.DataType object at 0x7666fc2f65f0>; kwargs: dimensions=256

app/services/deeplake_service.py:189: StorageException
----------------------------- Captured stdout call -----------------------------
{"name": "test-dataset", "error": "Array(): incompatible function arguments. The following argument types are supported:\n    1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType\n    2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType\n    3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType\n\nInvoked with: <deeplake._deeplake.types.DataType object at 0x7666fc2f65f0>; kwargs: dimensions=256", "event": "Failed to create dataset", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:04.590037Z"}
------------------------------ Captured log call -------------------------------
ERROR    DeepLakeService:deeplake_service.py:188 {"name": "test-dataset", "error": "Array(): incompatible function arguments. The following argument types are supported:\n    1. (dtype: Union[deeplake._deeplake.types.DataType, str], dimensions: int) -> deeplake._deeplake.types.DataType\n    2. (dtype: Union[deeplake._deeplake.types.DataType, str], shape: list[int]) -> deeplake._deeplake.types.DataType\n    3. (dtype: Union[deeplake._deeplake.types.DataType, str]) -> deeplake._deeplake.types.DataType\n\nInvoked with: <deeplake._deeplake.types.DataType object at 0x7666fc2f65f0>; kwargs: dimensions=256", "event": "Failed to create dataset", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:04.590037Z"}
___________________ TestDeepLakeService.test_search_vectors ____________________

self = <app.services.deeplake_service.DeepLakeService object at 0x7666fd441d50>
dataset_id = 'test-dataset', query_vector = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]
options = SearchOptions(top_k=10, threshold=None, metric_type=None, include_content=True, include_metadata=True, filters=None, deduplicate=False, group_by_document=False, rerank=False, ef_search=None, nprobe=None, max_distance=None, min_score=None)
tenant_id = 'default'

    async def search_vectors(
        self,
        dataset_id: str,
        query_vector: List[float],
        options: SearchOptions,
        tenant_id: Optional[str] = None
    ) -> SearchResponse:
        """Search for similar vectors."""
        start_time = time.time()
        dataset_key = self._get_dataset_key(dataset_id, tenant_id)
        dataset_path = self._get_dataset_path(dataset_id, tenant_id)
    
        if not os.path.exists(dataset_path):
            raise DatasetNotFoundException(dataset_id, tenant_id)
    
        try:
            # Load dataset
            if dataset_key not in self.datasets:
                self.datasets[dataset_key] = await self._load_dataset(dataset_path, read_only=True)
    
            dataset = self.datasets[dataset_key]
    
            # Validate query vector dimensions using our metadata
            dataset_info = await self._load_dataset_metadata(dataset_path)
            expected_dimensions = dataset_info.get('dimensions', 0)
            if len(query_vector) != expected_dimensions:
                raise InvalidVectorDimensionsException(expected_dimensions, len(query_vector))
    
            # Perform search
            query_embedding = np.array(query_vector, dtype=np.float32)
    
            # Use Deep Lake's search functionality
            loop = asyncio.get_event_loop()
>           search_results = await loop.run_in_executor(
                self.executor,
                lambda: dataset.search(
                    embedding=query_embedding,
                    k=options.top_k,
                    distance_metric=options.metric_type or dataset_info.get('metric_type', 'cosine'),
                    return_tensors=['id', 'document_id', 'chunk_id', 'embedding', 'content', 'metadata', 'created_at', 'updated_at']
                )
            )

app/services/deeplake_service.py:456: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/concurrent/futures/thread.py:59: in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: dataset.search(
                ^^^^^^^^^^^^^^
            embedding=query_embedding,
            k=options.top_k,
            distance_metric=options.metric_type or dataset_info.get('metric_type', 'cosine'),
            return_tensors=['id', 'document_id', 'chunk_id', 'embedding', 'content', 'metadata', 'created_at', 'updated_at']
        )
    )
E   AttributeError: 'deeplake._deeplake.Dataset' object has no attribute 'search'

app/services/deeplake_service.py:458: AttributeError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_deeplake_service.TestDeepLakeService object at 0x7666fd6a6f50>
deeplake_service = <app.services.deeplake_service.DeepLakeService object at 0x7666fd441d50>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}
test_search_data = {'options': {'include_content': True, 'include_metadata': True, 'top_k': 10}, 'query_vector': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]}

    async def test_search_vectors(self, deeplake_service: DeepLakeService, test_dataset_data, test_vector_data, test_search_data):
        """Test vector search."""
        # Create dataset and insert vector
        dataset_create = DatasetCreate(**test_dataset_data)
        dataset = await deeplake_service.create_dataset(dataset_create, "default")
    
        vector_create = VectorCreate(**test_vector_data)
        await deeplake_service.insert_vectors(
            dataset_id=dataset.id,
            vectors=[vector_create],
            tenant_id="default"
        )
    
        # Search for similar vectors
        search_options = SearchOptions(**test_search_data["options"])
>       result = await deeplake_service.search_vectors(
            dataset_id=dataset.id,
            query_vector=test_search_data["query_vector"],
            options=search_options,
            tenant_id="default"
        )

tests/unit/test_deeplake_service.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <app.services.deeplake_service.DeepLakeService object at 0x7666fd441d50>
dataset_id = 'test-dataset', query_vector = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]
options = SearchOptions(top_k=10, threshold=None, metric_type=None, include_content=True, include_metadata=True, filters=None, deduplicate=False, group_by_document=False, rerank=False, ef_search=None, nprobe=None, max_distance=None, min_score=None)
tenant_id = 'default'

    async def search_vectors(
        self,
        dataset_id: str,
        query_vector: List[float],
        options: SearchOptions,
        tenant_id: Optional[str] = None
    ) -> SearchResponse:
        """Search for similar vectors."""
        start_time = time.time()
        dataset_key = self._get_dataset_key(dataset_id, tenant_id)
        dataset_path = self._get_dataset_path(dataset_id, tenant_id)
    
        if not os.path.exists(dataset_path):
            raise DatasetNotFoundException(dataset_id, tenant_id)
    
        try:
            # Load dataset
            if dataset_key not in self.datasets:
                self.datasets[dataset_key] = await self._load_dataset(dataset_path, read_only=True)
    
            dataset = self.datasets[dataset_key]
    
            # Validate query vector dimensions using our metadata
            dataset_info = await self._load_dataset_metadata(dataset_path)
            expected_dimensions = dataset_info.get('dimensions', 0)
            if len(query_vector) != expected_dimensions:
                raise InvalidVectorDimensionsException(expected_dimensions, len(query_vector))
    
            # Perform search
            query_embedding = np.array(query_vector, dtype=np.float32)
    
            # Use Deep Lake's search functionality
            loop = asyncio.get_event_loop()
            search_results = await loop.run_in_executor(
                self.executor,
                lambda: dataset.search(
                    embedding=query_embedding,
                    k=options.top_k,
                    distance_metric=options.metric_type or dataset_info.get('metric_type', 'cosine'),
                    return_tensors=['id', 'document_id', 'chunk_id', 'embedding', 'content', 'metadata', 'created_at', 'updated_at']
                )
            )
    
            # Process results
            results = []
            for i, result in enumerate(search_results):
                try:
                    vector_data = {
                        'id': result['id'].data()[0] if result['id'].data() else '',
                        'document_id': result['document_id'].data()[0] if result['document_id'].data() else '',
                        'chunk_id': result['chunk_id'].data()[0] if result['chunk_id'].data() else '',
                        'values': result['embedding'].data()[0].tolist() if result['embedding'].data() else [],
                        'content': result['content'].data()[0] if result['content'].data() else '',
                        'metadata': result['metadata'].data()[0] if result['metadata'].data() else {},
                        'created_at': datetime.fromisoformat(result['created_at'].data()[0]) if result['created_at'].data() else datetime.now(timezone.utc),
                        'updated_at': datetime.fromisoformat(result['updated_at'].data()[0]) if result['updated_at'].data() else datetime.now(timezone.utc),
                    }
    
                    # Calculate similarity score
                    distance = np.linalg.norm(query_embedding - np.array(vector_data['values']))
                    score = 1.0 / (1.0 + distance) if distance > 0 else 1.0
    
                    vector_response = VectorResponse(
                        id=vector_data['id'],
                        dataset_id=dataset_id,
                        document_id=vector_data['document_id'],
                        chunk_id=vector_data['chunk_id'],
                        values=vector_data['values'],
                        content=vector_data['content'] if options.include_content else None,
                        metadata=vector_data['metadata'] if options.include_metadata else {},
                        dimensions=len(vector_data['values']),
                        created_at=vector_data['created_at'],
                        updated_at=vector_data['updated_at'],
                        tenant_id=tenant_id
                    )
    
                    results.append(SearchResultItem(
                        vector=vector_response,
                        score=float(score),
                        distance=float(distance),
                        rank=i + 1
                    ))
    
                except Exception as e:
                    self.logger.warning("Failed to process search result", index=i, error=str(e))
                    continue
    
            query_time = (time.time() - start_time) * 1000
    
            stats = SearchStats(
                vectors_scanned=len(dataset),
                index_hits=len(results),
                filtered_results=len(results),
                database_time_ms=query_time,
                post_processing_time_ms=0.0
            )
    
            self.logger.info(
                "Vector search completed",
                dataset_id=dataset_id,
                results_count=len(results),
                query_time_ms=query_time
            )
    
            return SearchResponse(
                results=results,
                total_found=len(results),
                has_more=False,
                query_time_ms=query_time,
                stats=stats
            )
    
        except Exception as e:
            self.logger.error("Failed to search vectors", dataset_id=dataset_id, error=str(e))
>           raise StorageException(f"Failed to search vectors: {str(e)}", "search_vectors")
E           app.models.exceptions.StorageException: Failed to search vectors: 'deeplake._deeplake.Dataset' object has no attribute 'search'

app/services/deeplake_service.py:537: StorageException
----------------------------- Captured stdout call -----------------------------
{"dataset_id": "test-dataset", "error": "'deeplake._deeplake.Dataset' object has no attribute 'search'", "event": "Failed to search vectors", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:04.681171Z"}
------------------------------ Captured log call -------------------------------
ERROR    DeepLakeService:deeplake_service.py:536 {"dataset_id": "test-dataset", "error": "'deeplake._deeplake.Dataset' object has no attribute 'search'", "event": "Failed to search vectors", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:04.681171Z"}
____________ TestDatasetEndpoints.test_create_dataset_unauthorized _____________

self = <tests.unit.test_http_api.TestDatasetEndpoints object at 0x7666fd52d450>
client = <starlette.testclient.TestClient object at 0x7666fc3b5470>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}

    def test_create_dataset_unauthorized(self, client: TestClient, test_dataset_data):
        """Test creating dataset without authorization."""
        response = client.post("/api/v1/datasets/", json=test_dataset_data)
>       assert response.status_code == 401
E       assert 500 == 401
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:43: AssertionError
_____________ TestVectorEndpoints.test_insert_vector_unauthorized ______________

self = <tests.unit.test_http_api.TestVectorEndpoints object at 0x7666fd52d090>
client = <starlette.testclient.TestClient object at 0x7666fc390e50>
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}

    def test_insert_vector_unauthorized(self, client: TestClient, test_vector_data):
        """Test inserting vector without authorization."""
        response = client.post(
            "/api/v1/datasets/test-dataset/vectors/",
            json=test_vector_data
        )
>       assert response.status_code == 401
E       assert 201 == 401
E        +  where 201 = <Response [201 Created]>.status_code

tests/unit/test_http_api.py:109: AssertionError
__________ TestVectorEndpoints.test_insert_vector_nonexistent_dataset __________

self = <tests.unit.test_http_api.TestVectorEndpoints object at 0x7666fd52d6d0>
client = <starlette.testclient.TestClient object at 0x7666fc3b6890>
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}
auth_headers = {'Authorization': 'ApiKey y7Qc2Fx_W3gDypK0bLy_VVUvqn8zZqtzIZjaoeBcqG4', 'Content-Type': 'application/json'}

    def test_insert_vector_nonexistent_dataset(self, client: TestClient, test_vector_data, auth_headers):
        """Test inserting vector into non-existent dataset."""
        response = client.post(
            "/api/v1/datasets/nonexistent/vectors/",
            json=test_vector_data,
            headers=auth_headers
        )
>       assert response.status_code == 404
E       assert 201 == 404
E        +  where 201 = <Response [201 Created]>.status_code

tests/unit/test_http_api.py:118: AssertionError
___________ TestVectorEndpoints.test_vector_operations_with_dataset ____________

self = <tests.unit.test_http_api.TestVectorEndpoints object at 0x7666fd519a70>
client = <starlette.testclient.TestClient object at 0x7666fc3b67b0>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}
auth_headers = {'Authorization': 'ApiKey rp_piaVsXWTvUYjvfXUeNgLRYKn0vOatYWlB9UymgGg', 'Content-Type': 'application/json'}

    def test_vector_operations_with_dataset(self, client: TestClient, test_dataset_data, test_vector_data, auth_headers):
        """Test vector operations with a real dataset."""
        # Create dataset first
        create_response = client.post(
            "/api/v1/datasets/",
            json=test_dataset_data,
            headers=auth_headers
        )
        assert create_response.status_code == 201
        dataset = create_response.json()
        dataset_id = dataset["id"]
    
        # Insert single vector
        vector_response = client.post(
            f"/api/v1/datasets/{dataset_id}/vectors/",
            json=test_vector_data,
            headers=auth_headers
        )
        assert vector_response.status_code == 201
        vector_result = vector_response.json()
>       assert vector_result["inserted_count"] == 1
E       assert 0 == 1

tests/unit/test_http_api.py:140: AssertionError
_________________ TestSearchEndpoints.test_search_unauthorized _________________

self = <tests.unit.test_http_api.TestSearchEndpoints object at 0x7666fd52d810>
client = <starlette.testclient.TestClient object at 0x7666fc33bd90>
test_search_data = {'options': {'include_content': True, 'include_metadata': True, 'top_k': 10}, 'query_vector': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]}

    def test_search_unauthorized(self, client: TestClient, test_search_data):
        """Test search without authorization."""
        response = client.post(
            "/api/v1/datasets/test-dataset/search",
            json=test_search_data
        )
>       assert response.status_code == 401
E       assert 500 == 401
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:165: AssertionError
_____________ TestSearchEndpoints.test_search_nonexistent_dataset ______________

self = <tests.unit.test_http_api.TestSearchEndpoints object at 0x7666fd52d950>
client = <starlette.testclient.TestClient object at 0x7666fc393930>
test_search_data = {'options': {'include_content': True, 'include_metadata': True, 'top_k': 10}, 'query_vector': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]}
auth_headers = {'Authorization': 'ApiKey snH7D0Wl74treuR2ND10Wic9kkRJwoy36Hbl6VkpCgw', 'Content-Type': 'application/json'}

    def test_search_nonexistent_dataset(self, client: TestClient, test_search_data, auth_headers):
        """Test search in non-existent dataset."""
        response = client.post(
            "/api/v1/datasets/nonexistent/search",
            json=test_search_data,
            headers=auth_headers
        )
>       assert response.status_code == 404
E       assert 500 == 404
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:174: AssertionError
_________________ TestSearchEndpoints.test_search_with_dataset _________________

self = <tests.unit.test_http_api.TestSearchEndpoints object at 0x7666fd519ba0>
client = <starlette.testclient.TestClient object at 0x7666fd469550>
test_dataset_data = {'description': 'A test dataset for unit tests', 'dimensions': 128, 'index_type': 'default', 'metadata': {'test': 'true'}, ...}
test_vector_data = {'chunk_count': 1, 'chunk_id': 'test-chunk-1', 'chunk_index': 0, 'content': 'This is test content', ...}
test_search_data = {'options': {'include_content': True, 'include_metadata': True, 'top_k': 10}, 'query_vector': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...]}
auth_headers = {'Authorization': 'ApiKey zXHf-W4pefGHei-Hi5GOma9TInC7hOmhUFjiKRlCv88', 'Content-Type': 'application/json'}

    def test_search_with_dataset(self, client: TestClient, test_dataset_data, test_vector_data, test_search_data, auth_headers):
        """Test search with a real dataset."""
        # Create dataset
        create_response = client.post(
            "/api/v1/datasets/",
            json=test_dataset_data,
            headers=auth_headers
        )
        assert create_response.status_code == 201
        dataset = create_response.json()
        dataset_id = dataset["id"]
    
        # Insert vector
        client.post(
            f"/api/v1/datasets/{dataset_id}/vectors/",
            json=test_vector_data,
            headers=auth_headers
        )
    
        # Search
        search_response = client.post(
            f"/api/v1/datasets/{dataset_id}/search",
            json=test_search_data,
            headers=auth_headers
        )
>       assert search_response.status_code == 200
E       assert 400 == 200
E        +  where 400 = <Response [400 Bad Request]>.status_code

tests/unit/test_http_api.py:201: AssertionError
----------------------------- Captured stdout call -----------------------------
{"dataset_id": "test-dataset", "error": "'deeplake._deeplake.Dataset' object has no attribute 'search'", "event": "Failed to search vectors", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:05.201769Z"}
------------------------------ Captured log call -------------------------------
ERROR    DeepLakeService:deeplake_service.py:536 {"dataset_id": "test-dataset", "error": "'deeplake._deeplake.Dataset' object has no attribute 'search'", "event": "Failed to search vectors", "logger": "DeepLakeService", "level": "error", "timestamp": "2025-07-15T23:45:05.201769Z"}
_____________ TestSearchEndpoints.test_text_search_not_implemented _____________

self = <tests.unit.test_http_api.TestSearchEndpoints object at 0x7666fd519cd0>
client = <starlette.testclient.TestClient object at 0x7666fd4aa5f0>
auth_headers = {'Authorization': 'ApiKey TXl8GiLaxAnMG-ssLOspifXjPGvlvJCw2nEf_dGLvxY', 'Content-Type': 'application/json'}

    def test_text_search_not_implemented(self, client: TestClient, auth_headers):
        """Test text search returns not implemented."""
        # Create dataset first
        dataset_data = {
            "name": "text-search-test",
            "dimensions": 128,
            "metric_type": "cosine",
            "overwrite": True
        }
        create_response = client.post(
            "/api/v1/datasets/",
            json=dataset_data,
            headers=auth_headers
        )
        dataset_id = create_response.json()["id"]
    
        # Try text search
        text_search_data = {
            "query_text": "test query",
            "options": {"top_k": 10}
        }
        response = client.post(
            f"/api/v1/datasets/{dataset_id}/search/text",
            json=text_search_data,
            headers=auth_headers
        )
>       assert response.status_code == 501  # Not implemented
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 500 == 501
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:234: AssertionError
____________ TestAuthenticationAndAuthorization.test_no_auth_header ____________

self = <tests.unit.test_http_api.TestAuthenticationAndAuthorization object at 0x7666fd52da90>
client = <starlette.testclient.TestClient object at 0x7666fc345e10>

    def test_no_auth_header(self, client: TestClient):
        """Test request without auth header."""
        response = client.get("/api/v1/datasets/")
>       assert response.status_code == 401
E       assert 500 == 401
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:243: AssertionError
_________ TestAuthenticationAndAuthorization.test_admin_only_endpoint __________

self = <tests.unit.test_http_api.TestAuthenticationAndAuthorization object at 0x7666fd56ad50>
client = <starlette.testclient.TestClient object at 0x7666fc2a25f0>
auth_headers = {'Authorization': 'ApiKey U_tIFSHrRRcDA9Cn4KtuQtPOvARvPSmmsLoFuSbgE7g', 'Content-Type': 'application/json'}

    def test_admin_only_endpoint(self, client: TestClient, auth_headers):
        """Test admin-only endpoint access."""
        response = client.get("/api/v1/metrics", headers=auth_headers)
>       assert response.status_code == 200  # Should work with admin permissions
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 500 == 200
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/unit/test_http_api.py:265: AssertionError
=============================== warnings summary ===============================
.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:323: 10 warnings
  /home/jscharber/eng/deeplake-api/.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_debugger_trigger_invalid_data
  /home/jscharber/eng/deeplake-api/app/services/deeplake_service.py:370: RuntimeWarning: overflow encountered in cast
    'embedding': np.array(vector.values, dtype=np.float32),

tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_malformed_json
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_empty_request_body
tests/integration/test_vector_insert_api.py::TestInsertEndpointErrorHandling::test_invalid_content_type
tests/unit/test_http_api.py::TestErrorHandling::test_invalid_json
  /home/jscharber/eng/deeplake-api/.venv/lib/python3.13/site-packages/httpx/_models.py:408: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.
    headers, stream = encode_request(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.13.5-final-0 ________________

Name                                       Stmts   Miss  Cover   Missing
------------------------------------------------------------------------
app/__init__.py                                3      0   100%
app/api/__init__.py                            0      0   100%
app/api/grpc/__init__.py                       0      0   100%
app/api/grpc/handlers/__init__.py              0      0   100%
app/api/grpc/handlers/dataset_handler.py     134    134     0%   3-317
app/api/grpc/handlers/health_handler.py       44     44     0%   3-89
app/api/grpc/handlers/search_handler.py      118    118     0%   3-285
app/api/grpc/handlers/vector_handler.py      138    138     0%   3-312
app/api/grpc/server.py                        45     45     0%   3-108
app/api/http/__init__.py                       0      0   100%
app/api/http/dependencies.py                  95     13    86%   42, 52, 62, 72, 93, 147-148, 166, 188-191, 228
app/api/http/v1/__init__.py                    2      0   100%
app/api/http/v1/datasets.py                  144     70    51%   63-65, 76-79, 111-121, 145-146, 169-179, 197-235, 275-292, 329-346
app/api/http/v1/health.py                     73     28    62%   39-41, 47, 50-51, 75, 82-83, 122-126, 141-180
app/api/http/v1/search.py                     99     52    47%   60-65, 78-92, 95-96, 101-102, 107-108, 156-157, 181-231, 249-292
app/api/http/v1/vectors.py                   138     93    33%   71-93, 143-163, 181-203, 221-259, 278-309, 327-361, 379-402
app/config/__init__.py                         0      0   100%
app/config/logging.py                         37     15    59%   63, 67, 71, 75, 81-103
app/config/settings.py                        74      0   100%
app/main.py                                   98     46    53%   31-85, 139-142, 162-189, 198, 226-242, 264, 275-283
app/middleware/__init__.py                     0      0   100%
app/models/__init__.py                         0      0   100%
app/models/exceptions.py                      51      7    86%   45-46, 61, 82, 96, 103, 110
app/models/proto/__init__.py                   0      0   100%
app/models/schemas.py                        184      6    97%   104, 187, 189, 211-213
app/services/__init__.py                       0      0   100%
app/services/auth_service.py                 156     61    61%   117, 120, 128-130, 142-144, 152-158, 168, 175-178, 182-192, 197, 201, 211, 219-220, 272, 289-308, 312-325, 329-341
app/services/cache_service.py                146     79    46%   29-36, 40-42, 50-61, 69-77, 81-90, 97-106, 110-118, 125-142, 146-149, 160, 167, 196-197, 213-214, 218-219, 223-225
app/services/deeplake_service.py             417    210    50%   52-54, 61, 67, 77, 83-85, 124, 206, 229-231, 253, 261-266, 270-272, 284, 291, 304-306, 322, 355-356, 381, 385-387, 419-421, 441, 449, 467-527, 546-598, 608-646, 655-691, 701-760, 764-770, 774-801, 805-835, 839-862, 869-870, 872-873, 879-880, 893-904, 923-924, 926-927
app/services/metrics_service.py              111     33    70%   212-220, 228-236, 277-297, 311, 315, 343, 347, 352-363, 370, 402-404, 411-412, 414-415, 423-424, 428
app/utils/__init__.py                          0      0   100%
------------------------------------------------------------------------
TOTAL                                       2307   1192    48%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml
Required test coverage of 25% reached. Total coverage: 48.33%
=========================== short test summary info ============================
FAILED tests/integration/test_end_to_end.py::TestEndToEnd::test_complete_workflow
FAILED tests/integration/test_end_to_end.py::TestEndToEnd::test_error_scenarios
FAILED tests/integration/test_end_to_end.py::TestEndToEnd::test_authentication_flows
FAILED tests/integration/test_end_to_end.py::TestEndToEnd::test_tenant_isolation
FAILED tests/integration/test_end_to_end.py::TestPerformance::test_large_batch_insert
FAILED tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_successful_single_vector_insert
FAILED tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_vector_insert_with_metadata
FAILED tests/integration/test_vector_insert_api.py::TestVectorInsertEndpoints::test_vector_insert_with_custom_id
FAILED tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_multiple_vectors
FAILED tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_with_skip_existing
FAILED tests/integration/test_vector_insert_api.py::TestVectorBatchInsertEndpoints::test_batch_insert_mixed_valid_invalid
FAILED tests/unit/test_deeplake_service.py::TestDeepLakeService::test_list_datasets
FAILED tests/unit/test_deeplake_service.py::TestDeepLakeService::test_insert_vectors
FAILED tests/unit/test_deeplake_service.py::TestDeepLakeService::test_insert_vector_wrong_dimensions
FAILED tests/unit/test_deeplake_service.py::TestDeepLakeService::test_search_vectors
FAILED tests/unit/test_http_api.py::TestDatasetEndpoints::test_create_dataset_unauthorized
FAILED tests/unit/test_http_api.py::TestVectorEndpoints::test_insert_vector_unauthorized
FAILED tests/unit/test_http_api.py::TestVectorEndpoints::test_insert_vector_nonexistent_dataset
FAILED tests/unit/test_http_api.py::TestVectorEndpoints::test_vector_operations_with_dataset
FAILED tests/unit/test_http_api.py::TestSearchEndpoints::test_search_unauthorized
FAILED tests/unit/test_http_api.py::TestSearchEndpoints::test_search_nonexistent_dataset
FAILED tests/unit/test_http_api.py::TestSearchEndpoints::test_search_with_dataset
FAILED tests/unit/test_http_api.py::TestSearchEndpoints::test_text_search_not_implemented
FAILED tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_no_auth_header
FAILED tests/unit/test_http_api.py::TestAuthenticationAndAuthorization::test_admin_only_endpoint
================== 25 failed, 35 passed, 15 warnings in 2.67s ==================
