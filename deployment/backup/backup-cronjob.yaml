apiVersion: batch/v1
kind: CronJob
metadata:
  name: deeplake-backup
  namespace: deeplake
  labels:
    app: deeplake-backup
    component: backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  concurrencyPolicy: Forbid  # Don't allow overlapping backup jobs
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  suspend: false
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: deeplake-backup
            component: backup
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: deeplake-api:latest
            command:
            - python
            - /app/scripts/disaster_recovery.py
            - create
            - --type
            - full
            - --description
            - "Automated daily backup"
            env:
            - name: BACKUP_ENABLED
              value: "true"
            - name: BACKUP_STORAGE
              value: "s3"
            - name: BACKUP_S3_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3-bucket
            - name: BACKUP_S3_REGION
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3-region
            - name: BACKUP_RETENTION_DAYS
              value: "90"
            - name: BACKUP_COMPRESSION
              value: "true"
            - name: BACKUP_ENCRYPTION
              value: "true"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: redis-credentials
                  key: url
            - name: DEEPLAKE_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deeplake-credentials
                  key: token
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            - name: temp-storage
              mountPath: /tmp
            resources:
              requests:
                memory: "512Mi"
                cpu: "250m"
              limits:
                memory: "2Gi"
                cpu: "1000m"
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          - name: temp-storage
            emptyDir:
              sizeLimit: "10Gi"
          
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: deeplake-backup-cleanup
  namespace: deeplake
  labels:
    app: deeplake-backup-cleanup
    component: backup
spec:
  schedule: "0 3 * * 0"  # Weekly on Sunday at 3 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  suspend: false
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: deeplake-backup-cleanup
            component: backup
        spec:
          restartPolicy: OnFailure
          containers:
          - name: cleanup
            image: deeplake-api:latest
            command:
            - python
            - /app/scripts/disaster_recovery.py
            - cleanup
            env:
            - name: BACKUP_ENABLED
              value: "true"
            - name: BACKUP_STORAGE
              value: "s3"
            - name: BACKUP_S3_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3-bucket
            - name: BACKUP_S3_REGION
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3-region
            - name: BACKUP_RETENTION_DAYS
              value: "90"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: redis-credentials
                  key: url
            - name: DEEPLAKE_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deeplake-credentials
                  key: token
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "250m"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-config
  namespace: deeplake
data:
  s3-bucket: "deeplake-prod-backups"
  s3-region: "us-east-1"
  s3-prefix: "deeplake-backups"
  retention-days: "90"
  compression: "true"
  encryption: "true"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: deeplake
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi

---
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
  namespace: deeplake
type: Opaque
data:
  # Base64 encoded AWS credentials
  access-key-id: <base64-encoded-access-key>
  secret-access-key: <base64-encoded-secret-key>

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: deeplake-backup
  namespace: deeplake
  labels:
    app: deeplake-backup
spec:
  selector:
    matchLabels:
      app: deeplake-backup
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics